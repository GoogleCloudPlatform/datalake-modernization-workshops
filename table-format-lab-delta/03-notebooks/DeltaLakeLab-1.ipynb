{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4529a790-8425-4600-841a-4b094b82eaa8",
   "metadata": {},
   "source": [
    "# Delta Lake Lab \n",
    "## Unit 1: Create a base Parquet table\n",
    "Create a base table in Parquet, off of the Kaggle Lending Club Loan dataset, preloaded into your GCS data bucket in directory parquet-source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31c4fd-d465-4f52-8e56-3775bf499abc",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1321bce9-178c-4065-8187-0a5728c1a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import month, date_format\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed295a74-ed1d-4b5d-831a-1b5dcf73c36f",
   "metadata": {},
   "source": [
    "### 2. Create a Spark session powered by Cloud Dataproc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b383d5ab-a0b9-45ab-a232-34d88f2a0065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/01 21:23:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gdpic-srvls-session-f2655fc3-8238-494e-b100-47a785085db6-m.us-central1-b.c.delta-lake-lab.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://gdpic-srvls-session-f2655fc3-8238-494e-b100-47a785085db6-m:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f62792bbfa0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Loan Analysis').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd13e6-f3f5-4f2c-b4fc-d7e2660c6206",
   "metadata": {},
   "source": [
    "### 3. Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5596e31b-749a-4702-8879-6f05f9ff0c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID:  delta-lake-lab\n"
     ]
    }
   ],
   "source": [
    "project_id_output = !gcloud config list --format \"value(core.project)\" 2>/dev/null\n",
    "PROJECT_ID = project_id_output[0]\n",
    "print(\"PROJECT_ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "471c6743-a058-462b-851a-f34323f36243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_NAME:  delta-lake-lab\n"
     ]
    }
   ],
   "source": [
    "project_name_output = !gcloud projects describe $PROJECT_ID | grep name | cut -d':' -f2 | xargs\n",
    "PROJECT_NAME = project_name_output[0]\n",
    "print(\"PROJECT_NAME: \", PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61929dc6-a083-433c-8a13-3d39d9c4a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_NUMBER:  885979867746\n"
     ]
    }
   ],
   "source": [
    "project_number_output = !gcloud projects describe $PROJECT_ID | grep projectNumber | cut -d':' -f2 | xargs\n",
    "PROJECT_NUMBER = project_number_output[0]\n",
    "print(\"PROJECT_NUMBER: \", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_NAME = \"akhjain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b79fd2-5243-41a4-ae87-e7f9dd87cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LAKE_ROOT_PATH= f\"gs://dll-data-bucket-{PROJECT_NUMBER}-{ACCOUNT_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0ec69d-c609-4fc5-9bdf-80025defb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_SOURCE_FQ_GCS_PATH = f\"{DATA_LAKE_ROOT_PATH}/parquet-source/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ef5b6-3db7-42da-bcb8-ef7b5efece68",
   "metadata": {},
   "source": [
    "### 4. Explore the raw loans data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59768a80-65cd-47ab-8870-f9e7c82b2811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dll-data-bucket-885979867746/parquet-source/:\n",
      "gs://dll-data-bucket-885979867746/parquet-source/loans_raw_1.snappy.parquet\n",
      "gs://dll-data-bucket-885979867746/parquet-source/loans_raw_2.snappy.parquet\n",
      "gs://dll-data-bucket-885979867746/parquet-source/loans_raw_3.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -r $DATA_LAKE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28c3a11-793a-4a31-a00a-1582c0c04431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rawDF = spark.read.parquet(DATA_LAKE_ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35520d8-4d9d-4d72-b033-306c742d892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- member_id: string (nullable = true)\n",
      " |-- loan_amnt: float (nullable = true)\n",
      " |-- funded_amnt: integer (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: string (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: float (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- delinq_2yrs: float (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- inq_last_6mths: string (nullable = true)\n",
      " |-- mths_since_last_delinq: integer (nullable = true)\n",
      " |-- mths_since_last_record: string (nullable = true)\n",
      " |-- open_acc: integer (nullable = true)\n",
      " |-- pub_rec: integer (nullable = true)\n",
      " |-- revol_bal: integer (nullable = true)\n",
      " |-- revol_util: string (nullable = true)\n",
      " |-- total_acc: float (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- out_prncp: string (nullable = true)\n",
      " |-- out_prncp_inv: double (nullable = true)\n",
      " |-- total_pymnt: string (nullable = true)\n",
      " |-- total_pymnt_inv: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- total_rec_int: double (nullable = true)\n",
      " |-- total_rec_late_fee: double (nullable = true)\n",
      " |-- recoveries: double (nullable = true)\n",
      " |-- collection_recovery_fee: double (nullable = true)\n",
      " |-- last_pymnt_d: string (nullable = true)\n",
      " |-- last_pymnt_amnt: string (nullable = true)\n",
      " |-- next_pymnt_d: string (nullable = true)\n",
      " |-- last_credit_pull_d: string (nullable = true)\n",
      " |-- collections_12_mths_ex_med: string (nullable = true)\n",
      " |-- mths_since_last_major_derog: string (nullable = true)\n",
      " |-- policy_code: string (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- annual_inc_joint: string (nullable = true)\n",
      " |-- dti_joint: double (nullable = true)\n",
      " |-- verification_status_joint: string (nullable = true)\n",
      " |-- acc_now_delinq: integer (nullable = true)\n",
      " |-- tot_coll_amt: integer (nullable = true)\n",
      " |-- tot_cur_bal: integer (nullable = true)\n",
      " |-- open_acc_6m: integer (nullable = true)\n",
      " |-- open_il_6m: integer (nullable = true)\n",
      " |-- open_il_12m: integer (nullable = true)\n",
      " |-- open_il_24m: integer (nullable = true)\n",
      " |-- mths_since_rcnt_il: integer (nullable = true)\n",
      " |-- total_bal_il: integer (nullable = true)\n",
      " |-- il_util: integer (nullable = true)\n",
      " |-- open_rv_12m: integer (nullable = true)\n",
      " |-- open_rv_24m: integer (nullable = true)\n",
      " |-- max_bal_bc: integer (nullable = true)\n",
      " |-- all_util: integer (nullable = true)\n",
      " |-- total_rev_hi_lim: integer (nullable = true)\n",
      " |-- inq_fi: integer (nullable = true)\n",
      " |-- total_cu_tl: integer (nullable = true)\n",
      " |-- inq_last_12m: integer (nullable = true)\n",
      " |-- acc_open_past_24mths: integer (nullable = true)\n",
      " |-- avg_cur_bal: integer (nullable = true)\n",
      " |-- bc_open_to_buy: integer (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      " |-- chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- delinq_amnt: integer (nullable = true)\n",
      " |-- mo_sin_old_il_acct: double (nullable = true)\n",
      " |-- mo_sin_old_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_tl: integer (nullable = true)\n",
      " |-- mort_acc: integer (nullable = true)\n",
      " |-- mths_since_recent_bc: integer (nullable = true)\n",
      " |-- mths_since_recent_bc_dlq: integer (nullable = true)\n",
      " |-- mths_since_recent_inq: integer (nullable = true)\n",
      " |-- mths_since_recent_revol_delinq: integer (nullable = true)\n",
      " |-- num_accts_ever_120_pd: integer (nullable = true)\n",
      " |-- num_actv_bc_tl: integer (nullable = true)\n",
      " |-- num_actv_rev_tl: integer (nullable = true)\n",
      " |-- num_bc_sats: integer (nullable = true)\n",
      " |-- num_bc_tl: integer (nullable = true)\n",
      " |-- num_il_tl: integer (nullable = true)\n",
      " |-- num_op_rev_tl: integer (nullable = true)\n",
      " |-- num_rev_accts: integer (nullable = true)\n",
      " |-- num_rev_tl_bal_gt_0: integer (nullable = true)\n",
      " |-- num_sats: integer (nullable = true)\n",
      " |-- num_tl_120dpd_2m: integer (nullable = true)\n",
      " |-- num_tl_30dpd: integer (nullable = true)\n",
      " |-- num_tl_90g_dpd_24m: integer (nullable = true)\n",
      " |-- num_tl_op_past_12m: integer (nullable = true)\n",
      " |-- pct_tl_nvr_dlq: double (nullable = true)\n",
      " |-- percent_bc_gt_75: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: integer (nullable = true)\n",
      " |-- tax_liens: integer (nullable = true)\n",
      " |-- tot_hi_cred_lim: integer (nullable = true)\n",
      " |-- total_bal_ex_mort: integer (nullable = true)\n",
      " |-- total_bc_limit: integer (nullable = true)\n",
      " |-- total_il_high_credit_limit: integer (nullable = true)\n",
      " |-- revol_bal_joint: integer (nullable = true)\n",
      " |-- sec_app_earliest_cr_line: string (nullable = true)\n",
      " |-- sec_app_inq_last_6mths: integer (nullable = true)\n",
      " |-- sec_app_mort_acc: integer (nullable = true)\n",
      " |-- sec_app_open_acc: integer (nullable = true)\n",
      " |-- sec_app_revol_util: double (nullable = true)\n",
      " |-- sec_app_open_il_6m: integer (nullable = true)\n",
      " |-- sec_app_num_rev_accts: integer (nullable = true)\n",
      " |-- sec_app_chargeoff_within_12_mths: integer (nullable = true)\n",
      " |-- sec_app_collections_12_mths_ex_med: integer (nullable = true)\n",
      " |-- sec_app_mths_since_last_major_derog: integer (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- hardship_type: string (nullable = true)\n",
      " |-- hardship_reason: string (nullable = true)\n",
      " |-- hardship_status: string (nullable = true)\n",
      " |-- deferral_term: integer (nullable = true)\n",
      " |-- hardship_amount: double (nullable = true)\n",
      " |-- hardship_start_date: string (nullable = true)\n",
      " |-- hardship_end_date: string (nullable = true)\n",
      " |-- payment_plan_start_date: string (nullable = true)\n",
      " |-- hardship_length: integer (nullable = true)\n",
      " |-- hardship_dpd: integer (nullable = true)\n",
      " |-- hardship_loan_status: string (nullable = true)\n",
      " |-- orig_projected_additional_accrued_interest: double (nullable = true)\n",
      " |-- hardship_payoff_balance_amount: double (nullable = true)\n",
      " |-- hardship_last_payment_amount: double (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e870e4b9-0c83-49af-9590-074dfd503ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/01 21:23:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "rawDF=rawDF.na.drop(subset=[\"addr_state\"])\n",
    "rawDF.createOrReplaceTempView(\"loans_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "648aec0d-f1a2-461a-beff-8d815ac84e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n",
      "ANTLR Tool version 4.8 used for code generation does not match the current runtime version 4.9.3\n",
      "ANTLR Runtime version 4.8 used for parser compilation does not match the current runtime version 4.9.3\n",
      "[Stage 1:=======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+----------+\n",
      "|state|       loan_status|loan_count|\n",
      "+-----+------------------+----------+\n",
      "|   CA|        Fully Paid|     40488|\n",
      "|   CO|           Current|      7648|\n",
      "|   CO|        Fully Paid|      6379|\n",
      "|   NC|           Current|     10910|\n",
      "|   NC| Late (16-30 days)|       102|\n",
      "|   KY| Late (16-30 days)|        22|\n",
      "|   PA|Late (31-120 days)|       360|\n",
      "|   IN|        Fully Paid|      4011|\n",
      "|   ME|   In Grace Period|        19|\n",
      "|   WY|        Fully Paid|       628|\n",
      "|   CO|   In Grace Period|       117|\n",
      "|   ND|        Fully Paid|       178|\n",
      "|   DE|Late (31-120 days)|        29|\n",
      "|   TX|        Fully Paid|     21206|\n",
      "|   DC|        Fully Paid|       783|\n",
      "|   MN|       Charged Off|      1280|\n",
      "|   IL|   In Grace Period|       276|\n",
      "|   OK|   In Grace Period|        53|\n",
      "|   VT|       Charged Off|        86|\n",
      "|   TX|   In Grace Period|       544|\n",
      "+-----+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count total loans\n",
    "spark.sql(\"select addr_state as state,loan_status, count(*) as loan_count from loans_raw group by addr_state,loan_status\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a1b86b4-593c-4736-9394-9d8a38c89311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT addr_state)|\n",
      "+--------------------------+\n",
      "|52                        |\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How many distinct states?\n",
    "spark.sql(\"select count(distinct addr_state) from loans_raw\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617874f-eedc-4714-a8ce-03a17e943fbe",
   "metadata": {},
   "source": [
    "### 5. Cleanse the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a3a70a1-1d3b-4931-b6f5-42ba6dd51301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(addr_state='AZ'),\n",
       " Row(addr_state='SC'),\n",
       " Row(addr_state='LA'),\n",
       " Row(addr_state='MN'),\n",
       " Row(addr_state='NJ'),\n",
       " Row(addr_state='DC'),\n",
       " Row(addr_state='OR'),\n",
       " Row(addr_state='VA'),\n",
       " Row(addr_state='RI'),\n",
       " Row(addr_state='WY'),\n",
       " Row(addr_state='KY'),\n",
       " Row(addr_state='NH'),\n",
       " Row(addr_state='MI'),\n",
       " Row(addr_state='NV'),\n",
       " Row(addr_state='WI'),\n",
       " Row(addr_state='ID'),\n",
       " Row(addr_state='CA'),\n",
       " Row(addr_state='CT'),\n",
       " Row(addr_state='NE'),\n",
       " Row(addr_state='MT'),\n",
       " Row(addr_state='NC'),\n",
       " Row(addr_state='VT'),\n",
       " Row(addr_state='MD'),\n",
       " Row(addr_state='DE'),\n",
       " Row(addr_state='MO'),\n",
       " Row(addr_state='IL'),\n",
       " Row(addr_state='ME'),\n",
       " Row(addr_state='WA'),\n",
       " Row(addr_state='ND'),\n",
       " Row(addr_state='MS'),\n",
       " Row(addr_state='AL'),\n",
       " Row(addr_state='IN'),\n",
       " Row(addr_state='OH'),\n",
       " Row(addr_state='TN'),\n",
       " Row(addr_state='NM'),\n",
       " Row(addr_state='PA'),\n",
       " Row(addr_state='SD'),\n",
       " Row(addr_state='NY'),\n",
       " Row(addr_state='TX'),\n",
       " Row(addr_state='WV'),\n",
       " Row(addr_state='GA'),\n",
       " Row(addr_state='MA'),\n",
       " Row(addr_state='KS'),\n",
       " Row(addr_state='FL'),\n",
       " Row(addr_state='CO'),\n",
       " Row(addr_state='AK'),\n",
       " Row(addr_state='AR'),\n",
       " Row(addr_state='OK'),\n",
       " Row(addr_state='UT'),\n",
       " Row(addr_state='HI'),\n",
       " Row(addr_state='debt_consolidation'),\n",
       " Row(addr_state='IA')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct states\n",
    "spark.sql(\"select distinct addr_state from loans_raw\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e587ccd3-bde0-46e8-936d-c8ed13ac3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data with invalid states\n",
    "cleasedSubsettedDF=spark.sql(\"select * from loans_raw where addr_state not in ('531xx','debt_consolidation')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "655c3269-0d51-48f1-8dfd-f8e6d22f8def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed and subsetted row count=740120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:======================================>                   (6 + 3) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed and subsetted distinct state count=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Quick counts\n",
    "count1=cleasedSubsettedDF.count()\n",
    "print(f\"Cleansed and subsetted row count={count1}\")\n",
    "\n",
    "count2=cleasedSubsettedDF.select(\"addr_state\").distinct().count()\n",
    "print(f\"Cleansed and subsetted distinct state count={count2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174df43b-9978-447e-8529-cc8e43cfa05d",
   "metadata": {},
   "source": [
    "### 6. Persist the cleansed data to the data lake, as Parquet & create an external table definition on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b63ce4c6-d663-4750-8b02-87f0db676956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Persist the cleaned data\n",
    "cleasedSubsettedDF.coalesce(3).write.format(\"parquet\").mode(\"overwrite\").save(f\"{DATA_LAKE_ROOT_PATH}/parquet-cleansed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dc7ff40-23e4-4e6b-b3c8-bfb91f9e7e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thrift://10.87.192.20:9080'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if we are using the Dataproc Metastore\n",
    "spark.sparkContext._conf.get(\"spark.hive.metastore.uris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ec38945-9a15-4227-8858-1cd413592439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/spark/conf/ivysettings.xml will be used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a database if it does not exist already\n",
    "spark.sql(\"SHOW DATABASES;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f6b1726-2e4a-4a23-9901-d77dd21d5d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a database if it does not exist already\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS loan_db;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f547356-b878-4166-9ac1-523b570b6ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/01 21:25:00 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an external table defintion on the parquet files\n",
    "spark.sql(\"DROP TABLE IF EXISTS loan_db.loans_cleansed_parquet;\").show(truncate=False)\n",
    "spark.sql(f\"CREATE TABLE loan_db.loans_cleansed_parquet USING parquet LOCATION '{DATA_LAKE_ROOT_PATH}/parquet-cleansed';\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9104153c-d55f-4e5f-a4ec-37ea1919806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dll-data-bucket-885979867746/parquet-cleansed/:\n",
      "gs://dll-data-bucket-885979867746/parquet-cleansed/\n",
      "gs://dll-data-bucket-885979867746/parquet-cleansed/_SUCCESS\n",
      "gs://dll-data-bucket-885979867746/parquet-cleansed/part-00000-e954d40d-3eae-4602-9cd2-a326b8b18438-c000.snappy.parquet\n",
      "gs://dll-data-bucket-885979867746/parquet-cleansed/part-00001-e954d40d-3eae-4602-9cd2-a326b8b18438-c000.snappy.parquet\n",
      "\n",
      "gs://dll-data-bucket-885979867746/parquet-source/:\n",
      "gs://dll-data-bucket-885979867746/parquet-source/loans_raw_1.snappy.parquet\n",
      "gs://dll-data-bucket-885979867746/parquet-source/loans_raw_2.snappy.parquet\n",
      "gs://dll-data-bucket-885979867746/parquet-source/loans_raw_3.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Review what's in the data lake\n",
    "!gsutil ls -r $DATA_LAKE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f4f3e-a4df-46a1-b032-bf97bb209afb",
   "metadata": {},
   "source": [
    "### 7. Create a parquet table on the base parquet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "717e4782-0936-4f66-b60a-6404403ae7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "# Remove any residual files from potential prior run\n",
    "!gsutil rm -rf $DATA_LAKE_ROOT_PATH/parquet-consumable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "813f55f8-cff6-4e7a-9305-0eb8ff4803c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table in Parquet off of the cleansed raw data\n",
    "spark.sql(\"DROP TABLE IF EXISTS loan_db.loans_by_state_parquet;\").show(truncate=False)\n",
    "spark.sql(f\"CREATE TABLE loan_db.loans_by_state_parquet USING parquet LOCATION '{DATA_LAKE_ROOT_PATH}/parquet-consumable' AS SELECT addr_state, count(loan_status) as count FROM loan_db.loans_cleansed_parquet GROUP BY addr_state;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5c604b1-3ded-420d-8bf5-0c35e2c61aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------+-----------+\n",
      "|namespace|tableName             |isTemporary|\n",
      "+---------+----------------------+-----------+\n",
      "|loan_db  |loans_by_state_parquet|false      |\n",
      "|loan_db  |loans_cleansed_parquet|false      |\n",
      "|         |loans_raw             |false      |\n",
      "+---------+----------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the Dataproc metastore for the new table\n",
    "spark.sql(\"show tables from loan_db;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abdeedf7-cc40-47a4-a04e-1d751f114dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|addr_state|count |\n",
      "+----------+------+\n",
      "|AZ        |17257 |\n",
      "|SC        |9020  |\n",
      "|LA        |8728  |\n",
      "|MN        |13501 |\n",
      "|NJ        |27212 |\n",
      "|DC        |1789  |\n",
      "|OR        |8696  |\n",
      "|VA        |21271 |\n",
      "|RI        |3307  |\n",
      "|WY        |1635  |\n",
      "|KY        |7147  |\n",
      "|NH        |3594  |\n",
      "|MI        |19324 |\n",
      "|NV        |10446 |\n",
      "|WI        |9741  |\n",
      "|ID        |876   |\n",
      "|CA        |103526|\n",
      "|CT        |11338 |\n",
      "|NE        |2104  |\n",
      "|MT        |2082  |\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List some data\n",
    "spark.sql(\"select * from loan_db.loans_by_state_parquet\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cedac28-84b0-46e9-be98-ac78d8b75afc",
   "metadata": {},
   "source": [
    "### 8. Review what is in the data lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb00e7a-786e-42b5-b3be-f1fee47e883a",
   "metadata": {},
   "source": [
    "Review cell #8. There was just one directory - parquet-source. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb3ae7-119e-4e06-acb2-92eaba37f739",
   "metadata": {},
   "source": [
    "Next review cell #19. A directory called parquet-cleased was added. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b955539-15af-4209-9455-0a73ab096d5f",
   "metadata": {},
   "source": [
    "At the end of this notebook, we also have a parquet-cleansed directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad20e372-f574-4092-a535-0333432d457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://dll-data-bucket-885979867746/parquet-cleansed/:\n",
      "gs://dll-data-bucket-885979867746/parquet-cleansed/\n",
      "gs://dll-data-bucket-885979867746/parquet-cleansed/_SUCCESS\n",
      "gs://dll-data-bucket-885979867746/parquet-cleansed/part-00000-e954d40d-3eae-4602-9cd2-a326b8b18438-c000.snappy.parquet\n",
      "gs://dll-data-bucket-885979867746/parquet-cleansed/part-00001-e954d40d-3eae-4602-9cd2-a326b8b18438-c000.snappy.parquet\n",
      "\n",
      "gs://dll-data-bucket-885979867746/parquet-consumable/:\n",
      "gs://dll-data-bucket-885979867746/parquet-consumable/\n",
      "gs://dll-data-bucket-885979867746/parquet-consumable/_SUCCESS\n",
      "gs://dll-data-bucket-885979867746/parquet-consumable/part-00000-707d61b6-b488-4daa-b9c1-f1dc435eded2-c000.snappy.parquet\n",
      "\n",
      "gs://dll-data-bucket-885979867746/parquet-source/:\n",
      "gs://dll-data-bucket-885979867746/parquet-source/loans_raw_1.snappy.parquet\n",
      "gs://dll-data-bucket-885979867746/parquet-source/loans_raw_2.snappy.parquet\n",
      "gs://dll-data-bucket-885979867746/parquet-source/loans_raw_3.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -r $DATA_LAKE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f10e97-18fe-4396-bb0a-78de19163953",
   "metadata": {},
   "source": [
    "We will use the data under the parquet-consumable directory in the next unit, and create a Delta table off of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb89089-468f-49c1-993f-752d7a9f7619",
   "metadata": {},
   "source": [
    "### THIS CONCLUDES THIS UNIT. PROCEED TO THE NEXT NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "serverless_spark": "{\"name\":\"projects/delta-lake-lab/locations/us-central1/sessions/delta-lake-lab-13803\",\"uuid\":\"f2655fc3-8238-494e-b100-47a785085db6\",\"createTime\":\"2022-11-01T20:33:39.245614Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{\"endpoints\":{\"Spark History Server\":\"https://smmei2wdurb7nptroshcbbgeda-dot-us-central1.dataproc.googleusercontent.com/sparkhistory/?eventLogDirFilter=f2655fc3-8238-494e-b100-47a785085db6\"}},\"state\":\"ACTIVE\",\"stateTime\":\"2022-11-01T20:35:05.784678Z\",\"creator\":\"admin@akhanolkar.altostrat.com\",\"runtimeConfig\":{\"version\":\"2.0\",\"properties\":{\"spark:spark.jars.packages\":\"io.delta:delta-core_2.13:2.1.0\",\"spark:spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\"spark:spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.dynamicAllocation.executorAllocationRatio\":\"0.3\",\"spark:spark.eventLog.dir\":\"gs://dll-sphs-bucket-885979867746/f2655fc3-8238-494e-b100-47a785085db6/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"dll-lab-sa@delta-lake-lab.iam.gserviceaccount.com\",\"subnetworkUri\":\"spark-snet\",\"idleTtl\":\"14400s\"},\"peripheralsConfig\":{\"metastoreService\":\"projects/delta-lake-lab/locations/us-central1/services/dll-hms-885979867746\",\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/delta-lake-lab/regions/us-central1/clusters/dll-sphs-885979867746\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-11-01T20:33:39.245614Z\"}]}",
  "serverless_spark_kernel_name": "remote-4a2e3012d18a64755cb71093-pyspark",
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}