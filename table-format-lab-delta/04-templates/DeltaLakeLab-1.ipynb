{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4529a790-8425-4600-841a-4b094b82eaa8",
   "metadata": {},
   "source": [
    "# Delta Lake Lab \n",
    "## Unit 1: Create a base Parquet table\n",
    "Create a base table in Parquet, off of the Kaggle Lending Club Loan dataset, preloaded into your GCS data bucket in directory parquet-source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31c4fd-d465-4f52-8e56-3775bf499abc",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321bce9-178c-4065-8187-0a5728c1a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import month, date_format\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed295a74-ed1d-4b5d-831a-1b5dcf73c36f",
   "metadata": {},
   "source": [
    "### 2. Create a Spark session powered by Cloud Dataproc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383d5ab-a0b9-45ab-a232-34d88f2a0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Loan Analysis').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd13e6-f3f5-4f2c-b4fc-d7e2660c6206",
   "metadata": {},
   "source": [
    "### 3. Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596e31b-749a-4702-8879-6f05f9ff0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id_output = !gcloud config list --format \"value(core.project)\" 2>/dev/null\n",
    "PROJECT_ID = project_id_output[0]\n",
    "print(\"PROJECT_ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c6743-a058-462b-851a-f34323f36243",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name_output = !gcloud projects describe $PROJECT_ID | grep name | cut -d':' -f2 | xargs\n",
    "PROJECT_NAME = project_name_output[0]\n",
    "print(\"PROJECT_NAME: \", PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61929dc6-a083-433c-8a13-3d39d9c4a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_number_output = !gcloud projects describe $PROJECT_ID | grep projectNumber | cut -d':' -f2 | xargs\n",
    "PROJECT_NUMBER = project_number_output[0]\n",
    "print(\"PROJECT_NUMBER: \", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_NAME = \"YOUR_ACCOUNT_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b79fd2-5243-41a4-ae87-e7f9dd87cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LAKE_ROOT_PATH= f\"gs://dll-data-bucket-{PROJECT_NUMBER}-{ACCOUNT_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ec69d-c609-4fc5-9bdf-80025defb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_SOURCE_FQ_GCS_PATH = f\"{DATA_LAKE_ROOT_PATH}/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ef5b6-3db7-42da-bcb8-ef7b5efece68",
   "metadata": {},
   "source": [
    "### 4. Explore the raw loans data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59768a80-65cd-47ab-8870-f9e7c82b2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage ls --recursive $DATA_LAKE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c3a11-793a-4a31-a00a-1582c0c04431",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF = spark.read.parquet(DATA_LAKE_ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35520d8-4d9d-4d72-b033-306c742d892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870e4b9-0c83-49af-9590-074dfd503ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF=rawDF.na.drop(subset=[\"addr_state\"])\n",
    "rawDF.createOrReplaceTempView(\"loans_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648aec0d-f1a2-461a-beff-8d815ac84e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total loans\n",
    "spark.sql(\"select addr_state as state,loan_status, count(*) as loan_count from loans_raw group by addr_state,loan_status\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b86b4-593c-4736-9394-9d8a38c89311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many distinct states?\n",
    "spark.sql(\"select count(distinct addr_state) from loans_raw\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617874f-eedc-4714-a8ce-03a17e943fbe",
   "metadata": {},
   "source": [
    "### 5. Cleanse the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a70a1-1d3b-4931-b6f5-42ba6dd51301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct states\n",
    "spark.sql(\"select distinct addr_state from loans_raw\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587ccd3-bde0-46e8-936d-c8ed13ac3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data with invalid states\n",
    "cleasedSubsettedDF=spark.sql(\"select * from loans_raw where addr_state not in ('531xx','debt_consolidation')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c3269-0d51-48f1-8dfd-f8e6d22f8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick counts\n",
    "count1=cleasedSubsettedDF.count()\n",
    "print(f\"Cleansed and subsetted row count={count1}\")\n",
    "\n",
    "count2=cleasedSubsettedDF.select(\"addr_state\").distinct().count()\n",
    "print(f\"Cleansed and subsetted distinct state count={count2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174df43b-9978-447e-8529-cc8e43cfa05d",
   "metadata": {},
   "source": [
    "### 6. Persist the cleansed data to the data lake, as Parquet & create an external table definition on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ce4c6-d663-4750-8b02-87f0db676956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the cleaned data\n",
    "cleasedSubsettedDF.coalesce(3).write.format(\"parquet\").mode(\"overwrite\").save(f\"{DATA_LAKE_ROOT_PATH}/parquet-cleansed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7ff40-23e4-4e6b-b3c8-bfb91f9e7e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we are using the Dataproc Metastore\n",
    "spark.sparkContext._conf.get(\"spark.hive.metastore.uris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec38945-9a15-4227-8858-1cd413592439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database if it does not exist already\n",
    "spark.sql(\"SHOW DATABASES;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b1726-2e4a-4a23-9901-d77dd21d5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database if it does not exist already\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS \"+ ACCOUNT_NAME +\"_loan_db;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f547356-b878-4166-9ac1-523b570b6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an external table defintion on the parquet files\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+ ACCOUNT_NAME +\"_loan_db.loans_cleansed_parquet;\").show(truncate=False)\n",
    "spark.sql(f\"CREATE TABLE YOUR_ACCOUNT_NAME_loan_db.loans_cleansed_parquet USING parquet LOCATION '{DATA_LAKE_ROOT_PATH}/parquet-cleansed';\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104153c-d55f-4e5f-a4ec-37ea1919806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review what's in the data lake\n",
    "!gcloud storage ls --recursive $DATA_LAKE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f4f3e-a4df-46a1-b032-bf97bb209afb",
   "metadata": {},
   "source": [
    "### 7. Create a parquet table on the base parquet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e4782-0936-4f66-b60a-6404403ae7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any residual files from potential prior run\n",
    "!gcloud storage rm --recursive --continue-on-error $DATA_LAKE_ROOT_PATH/parquet-consumable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f55f8-cff6-4e7a-9305-0eb8ff4803c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table in Parquet off of the cleansed raw data\n",
    "spark.sql(\"DROP TABLE IF EXISTS \"+ ACCOUNT_NAME +\"_loan_db.loans_by_state_parquet;\").show(truncate=False)\n",
    "spark.sql(f\"CREATE TABLE YOUR_ACCOUNT_NAME_loan_db.loans_by_state_parquet USING parquet LOCATION '{DATA_LAKE_ROOT_PATH}/parquet-consumable' AS SELECT addr_state, count(loan_status) as count FROM YOUR_ACCOUNT_NAME_loan_db.loans_cleansed_parquet GROUP BY addr_state;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c604b1-3ded-420d-8bf5-0c35e2c61aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Dataproc metastore for the new table\n",
    "spark.sql(\"show tables from \"+ ACCOUNT_NAME +\"_loan_db;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdeedf7-cc40-47a4-a04e-1d751f114dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List some data\n",
    "spark.sql(\"select * from \"+ ACCOUNT_NAME +\"_loan_db.loans_by_state_parquet\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cedac28-84b0-46e9-be98-ac78d8b75afc",
   "metadata": {},
   "source": [
    "### 8. Review what is in the data lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb00e7a-786e-42b5-b3be-f1fee47e883a",
   "metadata": {},
   "source": [
    "Review cell #8. There was just one directory - parquet-source. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb3ae7-119e-4e06-acb2-92eaba37f739",
   "metadata": {},
   "source": [
    "Next review cell #19. A directory called parquet-cleased was added. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b955539-15af-4209-9455-0a73ab096d5f",
   "metadata": {},
   "source": [
    "At the end of this notebook, we also have a parquet-cleansed directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20e372-f574-4092-a535-0333432d457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage ls --recursive $DATA_LAKE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f10e97-18fe-4396-bb0a-78de19163953",
   "metadata": {},
   "source": [
    "We will use the data under the parquet-consumable directory in the next unit, and create a Delta table off of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb89089-468f-49c1-993f-752d7a9f7619",
   "metadata": {},
   "source": [
    "### THIS CONCLUDES THIS UNIT. PROCEED TO THE NEXT NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "serverless_spark": "{\"name\":\"projects/delta-lake-lab/locations/us-central1/sessions/delta-lake-lab-13803\",\"uuid\":\"f2655fc3-8238-494e-b100-47a785085db6\",\"createTime\":\"2022-11-01T20:33:39.245614Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{\"endpoints\":{\"Spark History Server\":\"https://smmei2wdurb7nptroshcbbgeda-dot-us-central1.dataproc.googleusercontent.com/sparkhistory/?eventLogDirFilter=f2655fc3-8238-494e-b100-47a785085db6\"}},\"state\":\"ACTIVE\",\"stateTime\":\"2022-11-01T20:35:05.784678Z\",\"creator\":\"admin@akhanolkar.altostrat.com\",\"runtimeConfig\":{\"version\":\"2.0\",\"properties\":{\"spark:spark.jars.packages\":\"io.delta:delta-core_2.13:2.1.0\",\"spark:spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\"spark:spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.dynamicAllocation.executorAllocationRatio\":\"0.3\",\"spark:spark.eventLog.dir\":\"gs://dll-sphs-bucket-885979867746/f2655fc3-8238-494e-b100-47a785085db6/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"dll-lab-sa@delta-lake-lab.iam.gserviceaccount.com\",\"subnetworkUri\":\"spark-snet\",\"idleTtl\":\"14400s\"},\"peripheralsConfig\":{\"metastoreService\":\"projects/delta-lake-lab/locations/us-central1/services/dll-hms-885979867746\",\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/delta-lake-lab/regions/us-central1/clusters/dll-sphs-885979867746\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-11-01T20:33:39.245614Z\"}]}",
  "serverless_spark_kernel_name": "remote-4a2e3012d18a64755cb71093-pyspark",
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
