{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The right tool for the job: combining pandas and SPARK\n",
    "\n",
    "When it comes to data analysis with python, `pandas` is a very popular library for data manipulation, however it is constrained to datasets that fit into memory on a single machine.\n",
    "\n",
    "When scale is needed, `Apache Spark` is a distributed computing system that can process large amounts of data quickly thanks to its scale-out architecture. Its is very common to combine different tools in the data analysis process, for instance, starting with `pandas` and bursting heavy analysis to `spark` as needed.\n",
    "\n",
    "With Google Cloud, it is possible to use both `pandas` and `spark` together to perform data analysis tasks, without increasing the complexity of the solution. **Google Cloud's Spark Serverless** is a fully managed service for running `spark` workloads on Google Cloud. It allows you to easily submit `spark` jobs to be executed on Cloud Dataproc, Google Cloud's fully managed service for running `spark` and  `hadoop` workloads.\n",
    "\n",
    "With **Google Cloud's Spark Serverless**, you don't need to worry about managing the underlying infrastructure or configuring `spark`  clusters. You simply submit your `spark`  job, and the service automatically provisions the necessary resources and executes the job on a managed `spark`  cluster. When the job is finished, the resources are automatically released, so you only pay for what you use.\n",
    "\n",
    "In this notebook we will explore a use case:\n",
    "\n",
    "* Use `pandas` to pre-process data: `pandas` is good at handling small to medium-sized data sets, so you can use it to perform initial data cleaning and manipulation. For example, you can use `pandas` to read data from `BigQuery`, filter the data, and create simple visualizations.\n",
    "\n",
    "* Use `spark` to scale up: Once you have pre-processed your data using `pandas`, you can use `spark` to scale up your analysis to handle larger data sets. `spark` can be used to perform distributed computing on a cluster of machines, which makes it well-suited for big data tasks. `spark` has a fundamental data structure called a `spark dataframe`, which is similar to a `pandas dataframe`. You can use `spark dataframe` to perform distributed computations on large data sets, and you can also convert `pandas dataframe` to `spark dataframe` using the `spark.createDataFrame method`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using pandas together with BigQuery\n",
    "\n",
    "Lets start by reading some data from `BigQuery` public datasets into a `pandas dataframe`. Vertex AI managed notebooks integration with `BigQuery` makes this process simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#@bigquery\n",
    "SELECT * FROM bigquery-public-data.chicago_crime.crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two lines are only necessary to run once.\n",
    "# Comment out otherwise for speed-up.\n",
    "from google.cloud.bigquery import Client, QueryJobConfig\n",
    "client = Client()\n",
    "\n",
    "query = \"\"\"SELECT * FROM bigquery-public-data.chicago_crime.crime\"\"\"\n",
    "job = client.query(query)\n",
    "df = job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the memory allocation for the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect the memory configuration for this notebook, note that we can always change the machine type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '$3==\"kB\"{$2=$2/1024^2;$3=\"GB\";} 1' /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '$3==\"kB\"{$2=$2/1024^2;$3=\"GB\";} 1' /proc/meminfo | grep MemFree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform some simple aggregations on the dataset, for transformations on small datasets `pandas` is a very expressive and rich tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_arrest = df.groupby(by=[\"primary_type\",\"arrest\"]).size().sort_values(ascending=False).rename(\"count\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the aggregation results using `seaborn` and `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "sns.barplot( y=\"primary_type\",x=\"count\" , data=count_arrest.iloc[:20, :], hue='arrest', color='red')\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"Type\",xlabel=\"Crimes\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform some (not very useful) expensive operation now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [df.join(df,on=\"unique_key\",rsuffix=\"_y\") for _ in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either ther kernel will die, or a OOM error will be thrown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scaling data analysis with SPARK serverless interactive\n",
    "\n",
    "Lets replicate the previous workload with `pySPARK` running on Google Cloud SPARK Serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Using pandas on SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.set_option(\"compute.default_index_type\", \"distributed\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change type to avoid TypeError: Type datetime64[ns, UTC] was not understood. when converting to pandas on spark dataframe\n",
    "# See https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/types.html for additional information\n",
    "df['date'] = pd.to_datetime(df.date).dt.tz_localize(None)\n",
    "df['updated_on'] = pd.to_datetime(df.updated_on).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf = ps.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_arrest_psdf = psdf.groupby(by=[\"primary_type\",\"arrest\"]).size().sort_values(ascending=False).rename(\"count\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "sns.barplot( y=\"primary_type\",x=\"count\" , data=count_arrest_psdf.to_pandas().iloc[:20, :], hue='arrest', color='green')\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"Type\",xlabel=\"Crimes\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf = [psdf.join(psdf.rename( columns = { \"unique_key\" : \"unique_key-{}\".format(randint(0,1000)) } ),on=\"unique_key\",rsuffix=\"_y\") for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Using SPARK Dataframe API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to read data directly from BigQuery storage using the spark BigQuery connector for spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.format('bigquery') \\\n",
    "  .option('table', 'bigquery-public-data:chicago_crime.crime') \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PySPARK dataframe API is pretty similar to the pandas one, so the code refactor is minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_count_arrest = spark_df.groupby(\"primary_type\",\"arrest\").count().orderBy(col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can swith between pandas dataframes and spark dataframes easily. Unlike pandas, in spark the execution is lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_count_arrest = spark_count_arrest.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_count_arrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5,5))\n",
    "sns.barplot( y=\"primary_type\",x=\"count\" , data=spark_count_arrest.iloc[:20, :], hue='arrest', color='blue')\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(ylabel=\"Type\",xlabel=\"Crimes\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets execute the heavy operation again, this time we will do it in a distributed a spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.join(spark_df,on=\"unique_key\").join(spark_df,on=\"unique_key\").join(spark_df,on=\"unique_key\").join(spark_df,on=\"unique_key\").join(spark_df,on=\"unique_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to not sature the node memory lets retrieve a small percentage of the processed data with the `sample` operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.sample(0.01).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "serverless_spark": "{\"name\":\"projects/velascoluis-dev-sandbox/locations/us-central1/sessions/spark_hackfest_lab02\",\"uuid\":\"54a66c34-9cac-4f49-bf94-08fdb5431c08\",\"createTime\":\"2022-12-28T17:00:54.134609Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{},\"state\":\"ACTIVE\",\"stateTime\":\"2022-12-28T17:02:11.857061Z\",\"creator\":\"velascoluis@velascoluis.altostrat.com\",\"runtimeConfig\":{\"version\":\"1.0.25\",\"properties\":{\"spark:spark.jars.packages\":\"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.25.2\",\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.dynamicAllocation.executorAllocationRatio\":\"0.3\"}},\"environmentConfig\":{\"executionConfig\":{\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/velascoluis-dev-sandbox/regions/us-central1/subnetworks/default\",\"idleTtl\":\"14400s\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-12-28T17:00:54.134609Z\"}]}",
  "serverless_spark_kernel_name": "remote-4e5b4dc29bd640f52df7f56a-pyspark",
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
